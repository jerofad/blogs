{
  
    
        "post0": {
            "title": "Title",
            "content": "Quick Introduction to Gaussian Mixture Model . In density estimation, we represent the data compactly using a density from a parametric family like Gaussian distribution or beta distribution. In using a Gaussian distribution for densty modelling, we seek to find the mean and variances of that represents such data compactly. Point estimation like Maximum Likelihood Estimation (MLE) or Maximum A posterior (MAP) can be used to find such mean and variance. . Most of the time, single model do fail in expressibility. In other words there is limitation in a modelling the data well. So we consider a mixture of such models. . Mixture Models . A mixture model is used to describe a distribution $P(x)$ by a convex combination of K base distributions. I.e we seek a number, K, model distributions that are combined with weights. Which can be represented mathematically below as . begin{align*} p(x) = sum_{k=1}^{K} pi_kP_k(x) end{align*}where $ pi_k$ represents the weights and with the condition that begin{align*} 0 leq pi_k leq 1 space , space sum_{k=1}^{K} pi_k = 1 end{align*} . Assuming the models are Gaussian, then we would be resulting into a Gaussian Mixture Model (GMM) which would be the focus of the tutorial. Other distributions that has a &quot;name&quot; could one way or the other be combined to have a mixture model examples are; Beta distribution, Gamma distribution and so on. newline Now our Gaussian Mixture Model would now be written mathematically as; begin{align*} p(x) = sum_{k=1}^{K} pi_k mathcal{N}(x| mu_k, Sigma_k) end{align*} . From above equation, we discover that our GMM now has 3 different parameters and they are $ theta = { pi_k, mu_k, Sigma_k }$. so what we need to do now is to find a closed form solution for the parameter $ theta$ using the MLE and MAP. Just for heads up, we really cannot find a closed form solution, but we would see why we can&#39;t get such a closed form solution and then discuss way around to find the parameter. . So given our data $X = { x_1, dots, x_N }$ and parameter $ theta$, we would have a likelihood as shown below: begin{align*} P(X| theta) = prod_{n=1}^{N}p(x_n| theta) end{align*} Assume our data is identically independently distributed (iid) then we would have a factorized likelihood as follows begin{align*} P(X| theta) = prod_{n=1}^{N} sum_{k=1}^{K} pi_k mathcal{N}(x_n| mu_k, Sigma_k) end{align*} So taking the logarithm of the likelihood as we always do in MLE, our resulting equation becomes: begin{align*} log P(X| theta) = sum_{n=1}^{N} log sum_{k=1}^{K} pi_k mathcal{N}(x_n| mu_k, Sigma_k) end{align*} Take a look at the last equation and find out how we could proceed analytically. We discovered that the logarithm function can&#39;t &quot;enter&quot; the sum. But if our K =1, its possible to move from there, but since we have multiple k&#39;s, we can not find a closed form solution. So how do we proceed to get the parameters $ theta$, we turn to an iterative scheme in finding the good parameters and Expectation Maximization (EM) algorithm is here for the bail. . A necessary condition for getting local optimum is that the derivative of the log likelihood with respect to each parameters must be zero. See the reference if you&#39;re interested in the detailed derivation of the maths behind it. Here I am just going to state the formula and we proceed. After derivation, we arrive at what we call responsibility which is closely related to the likelihood as follows: begin{align*} r_{nk} = frac{ pi_k mathcal{N}(x_n| mu_k, Sigma_k)}{ sum_{k=1}^{K} pi_k mathcal{N}(x_n| mu_k, Sigma_k)} end{align*} . An important check is that our responsibility must sum up to 1 across each data-points i.e $ sum_{k=1}^{K}r_{nk}= 1$ . Hence, $r_{nk}$ distributes a probability mass across the K components. The next line of action is to update the means and covariance matrix of each of the components using the responsibility $r_{nk}$ and the updated parameters are: begin{align*} boldsymbol mu_k^ text{new} &amp;= frac{ sum_{n = 1}^Nr_{nk} boldsymbol x_n}{ sum_{n =1}^N r_{nk}} ,, boldsymbol Sigma_k^ text{new}&amp;= frac{1}{N_k} sum_{n=1}^Nr_{nk}( boldsymbol x_n- boldsymbol mu_k)( boldsymbol x_n- boldsymbol mu_k)^ top ,, pi_k^ text{new} &amp;= frac{N_k}{N} end{align*} . The math details of the formula can be checked out at the reference section. The EM Algorithm can be written as follows: . Choose initial values for $ mu_k$, $ Sigma_k$, $ pi_k $ | Evaluate the responsibility $r_{nk}$ | Use the updated $r_{nk}$ to update the $ { pi_k, mu_k, Sigma_k }$ | . If convergence condition is not met, repeat step 2 and 3 . And so we are done with some theoretical description of Gaussian Mixture Model. . Let us see how to approach this via coding them up. We would approach this as a modelling approach and give reasons for some of the choices . #collapse-hide import numpy as np from scipy import stats from sklearn.cluster import AffinityPropagation from sklearn.cluster import KMeans from scipy.spatial.distance import cdist import matplotlib.pyplot as plt from matplotlib.patches import Ellipse . . Data . The data we are using is a geo-location data gathered from 30 students in various countries from Africa. You are given a two-dimensional dataset of about 30 geolocations (latitude/longitude) of birthplaces. The task here is to use GMM to model the data. . #collapse-hide data = [[6.14512, 7.14512], [6.5582, 3.3466], [5.4646, 10.0631], [14.1372, -16.0754], [35.6177, 5.0999], [0.2774244, 32.4783873], [5.56508, -0.1601], [15.65611, 32.48492], [14.7074454, -17.474397], [14.7765117, -17.3226387], [4.03106, 9.73532], [24.713139, 46.622186], [15.65792, 32.46231], [52.4378, -0.76], [13.441518, -6.268147], [4.1058, 9.7529], [10.1345, 14.7117], [5.56, -0.2057], [-1.8848, 29.7772], [3.9389, 9.7437], [7.4431, 3.9022], [7.8797, 2.7675], [21.54238, 39.19797], [10.633, 38.783], [-1.6166, 30.1166], [0.2408, 32.5523], [15.79255, 32.52596], [6.531475, 3.32405], [4.9535, 9.9382], [-1.9008438, 36.2843654]] # convert our list to an ndarray X = np.asarray(data) . . Modelling . In Model selection, we use the idea from Occam’s razor technique that says we should favour simple model over complicated ones when we have fewer datasets. One specific challenge in our case here is that the data points are few so it would be difficult getting a good density estimation. . My approach in modelling this data is to first use Affinity Propagation which is an algorithm that ”guess” the number of suitable clusters for the data. . #collapse-hide af = AffinityPropagation().fit(X) cluster_centers_indices = af.cluster_centers_indices_ labels = af.labels_ n_clusters_ = len(cluster_centers_indices) print(n_clusters_) . . 5 . This algorithm outputs 5 clusters as the optimal number of clusters. . The first thoughts that comes to mind in modelling a datasets like this is to consider using K-means clustering. . Let us use KMeans algorithm from scikit learn and use it as a model for data to figure out what is going on here. . kmeans = KMeans(n_clusters=5, random_state=0) def plot_kmeans(kmeans, X, n_clusters=5, rseed=0, ax=None): labels = kmeans.fit_predict(X) # plot the input data ax = ax or plt.gca() ax.axis(&#39;equal&#39;) ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap=&#39;viridis&#39;, zorder=2) # plot the representation of the KMeans model centers = kmeans.cluster_centers_ radii = [cdist(X[labels == i], [center]).max() for i, center in enumerate(centers)] for c, r in zip(centers, radii): ax.add_patch(plt.Circle(c, r, fc=&#39;#CCCCCC&#39;, lw=3, alpha=0.5, zorder=1)) plot_kmeans(kmeans, X) . Using K-Means is not a bad idea but then as seen above, it makes a hard clustering on the data and also the shape of the clusters is not most times intuitive. Als, we can see that some data points lies on the boundary of the shape of the clusters. . We consider a generalization of K- means which is the Gaussian Mixture Model (GMM). . Next is to try and also bring into account the variance represented in the clustering which GMM takes care of. In GMM we also do have to specify the number of components of Gaussian models to mix. In this case, I decided to use 4 clusters just a ±1 to the number of clusters we have before. . Modelling with GMM . The first step in our modelling is to initialize our parameters which are the means, covariance and responsibility (prior). $ mu_k$, $ Sigma_k$, $ pi_k $ . N, D = X.shape[0], X.shape[1] # Number of mixture models K = 2 . means = np.zeros((K, D)) covs = np.zeros((K, D, D)) priors = np.zeros((K, 1)) initialCovariance = np.cov(X.T) for i in range(0, K): means[i] = np.random.rand(D) #the initial mean for each gaussian is chosen randomly covs[i] = initialCovariance #the initial covariance of each cluster is the covariance of the data priors[i, 0] = 1.0 / K #the initial priors are uniformly distributed. initial_params = {&quot;mu&quot;: means, &quot;S&quot;:covs, &quot;pi&quot;:priors} . The second step in the algorithm is to evaluate the responsibility $r_{nk}$ which is the E-step . #collapse-show def E_step( mu, S, pi): &quot;&quot;&quot;Compute the E step of the EM algorithm. Arguments: mu -- component means, K x D array S -- component covariances, K x D x D array pi -- component weights, K x 1 array Returns: r_new -- updated component responsabilities, N x K array &quot;&quot;&quot; # Assert that all arguments have the right shape assert(mu.shape == (K, D) and S.shape == (K, D, D) and pi.shape == (K, 1)) r_new = np.zeros((N, K)) # Here we implement the E step and return updated responsabilities for k in range(K): S[k] = S[k] + 1e-6* np.eye(D) r_new[:, k] = pi[k] * stats.multivariate_normal.pdf(X, mu[k], S[k]) r_new = r_new / (r_new.sum()) assert(r_new.shape == (N,K)) return r_new . . The third step, M-step, uses the updated $r_{nk}$ from E-step is to update the $ { pi_k, mu_k, Sigma_k }$ given below as . #collapse-show def M_step(mu, r): &quot;&quot;&quot;Compute the M step of the EM algorithm. Arguments: mu -- previous component means, K x D array r -- previous component responsabilities, N x K array Returns: mu_new -- updated component means, K x D array S_new -- updated component covariances, K x D x D array pi_new -- updated component weights, K x 1 array &quot;&quot;&quot; assert(mu.shape == (K, D) and r.shape == (N, K)) mu_new = np.zeros((K,D)) S_new = np.zeros((K, D, D)) pi_new = np.zeros((K, 1)) # Task 2: implement the M step and return updated mixture parameters for i in range(0, K): pi_new[i, 0] = np.sum(r[:, i]) / N #update priors for j in range(0, N): #update means mu_new[i] += r[j, i] * X[j, :] mu_new[i] /= np.sum(r[:, i]) for i in range(0, K): for j in range(0, N): #update means vec = np.reshape(X[j, :] - mu_new[i, :], (D, 1)) S_new[i] += r[j, i] * np.multiply(vec, vec.T) #update covs S_new[i] /= np.sum(r[:, i]) assert(mu_new.shape == (K, D) and S_new.shape == (K, D, D) and pi_new.shape == (K, 1)) return mu_new, S_new, pi_new . . The next and final step is to implement the EM algorithm that helps us to train our Gaussian mixture model. . The training step basically involves running our E and M steps itereatively until a stopping criteria is reached. Our stopping criteria is dependent on the log-likelihood estimate considering that &quot;every step in the EM algorithm increases the log-likelihood function&quot; . #collapse-hide def train(initial_params): &quot;&quot;&quot;Fit a Gaussian Mixture Model (GMM) to the data in matrix X. Arguments: initial_params -- dictionary with fields &#39;mu&#39;, &#39;S&#39;, &#39;pi&#39; and &#39;K&#39; Returns: mu -- component means, K x D array S -- component covariances, K x D x D array pi -- component weights, K x 1 array r -- component responsabilities, N x K array &quot;&quot;&quot; # Assert that initial_params has all the necessary fields assert(all([k in initial_params for k in [&#39;mu&#39;, &#39;S&#39;, &#39;pi&#39;]])) mu = initial_params[&quot;mu&quot;] S = initial_params[&quot;S&quot;] pi = initial_params[&quot;pi&quot;] # Task 3: implement the EM loop to train the GMM num_Iter = 30 log_likelihoods = [] loglik_old =0 threshold = 0.001 i = 0 while i &lt; num_Iter: # Run The E -step r = E_step(mu, S, pi) # Run M-Step to update the parameters. mu, S, pi = M_step(mu, r) loglikelihood = 0.0 for l in range(N): s = 0 for j in range(K): S[j] = S[j] + 1e-6* np.eye(D) s += pi[j] * stats.multivariate_normal.pdf(X[l], mu[j], S[j]) loglikelihood += np.log(s) log_likelihoods.append(loglikelihood) # check for convergence if len(log_likelihoods) &lt; 2 : continue if np.abs(loglikelihood - log_likelihoods[-1]) &lt; threshold: print(&quot;Likelihood has converged !!&quot;) break i += 1 assert(mu.shape == (K, D) and S.shape == (K, D, D) and pi.shape == (K, 1) and r.shape == (N, K)) return mu, S, pi, r . . Now that we have writtent the code all that we need to find our parameters for the GMM, we can now train our model. . #collapse-show mu_, S_, pi_, r_ = train(initial_params) . . Likelihood has converged !! . We can now visualize our model by running GMM on our data. . def plot_ellipse(pos, cov, nstd=2, ax=None, **kwargs): def eigsorted(cov): vals, vecs = np.linalg.eigh(cov) order = vals.argsort()[::-1] return vals[order], vecs[:,order] if ax is None: ax = plt.gca() vals, vecs = eigsorted(cov) theta = np.degrees(np.arctan2(*vecs[:,0][::-1])) # Width and height are &quot;full&quot; widths, not radius width, height = 2 * nstd * np.sqrt(abs(vals)) ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwargs) ax.add_artist(ellip) return ellip def show(X, mu, cov): plt.cla() K = len(mu) # number of clusters colors = [&#39;b&#39;, &#39;k&#39;, &#39;g&#39;, &#39;c&#39;, &#39;m&#39;, &#39;y&#39;, &#39;r&#39;] plt.plot(X.T[0], X.T[1], &#39;m*&#39;) for k in range(K): plot_ellipse(mu[k], cov[k], alpha=0.6, color = colors[k % len(colors)]) . X.shape, mu_.shape, S_.shape . ((30, 2), (2, 2), (2, 2, 2)) . fig = plt.figure(figsize = (12, 12)) # fig.add_subplot(121) show(X, mu_, S_) . Now we observe that a data-point could belong to one or more clusters (soft clustering). Also we could see that in GMM model, the shape is different to the clustering. If we decide to increase the number of clusters to 5, then we would be having overlap datapoints. That is a data point could fall within two clusters. Hence, we have been able to provide a better modelling for our data. . But then model selection comes to play. How do we know the best number of three components of gaussian mixture to use. We use the Bayesian Information Criterion (BIC) to determine the optimal number of components. The figure below shows the result of using BIC on our Gaussian Mixture Models. .",
            "url": "https://jerofad.github.io/blogs/2020/03/12/GMM-Tutorial.html",
            "relUrl": "/2020/03/12/GMM-Tutorial.html",
            "date": " • Mar 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jerofad.github.io/blogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jerofad.github.io/blogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jerofad.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}